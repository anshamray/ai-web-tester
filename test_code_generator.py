#!/usr/bin/env python3
"""
Test Code Generation Agent

This agent takes test cases generated by WebAnalysisAgent and converts them
into executable automated test code using Playwright and pytest.
"""

import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from config import Config


class TestCodeGenerator:
    """Agent for generating automated test code from test cases"""
    
    def __init__(self):
        Config.validate()
        self.llm = ChatOpenAI(
            temperature=0.1,
            api_key=Config.OPENAI_API_KEY,
            model="gpt-4o-mini"
        )
    
    async def generate_test_code(self, report: Dict[str, Any], output_dir: str = "generated_tests") -> Dict[str, Any]:
        """Generate automated test code from analysis report"""
        
        print(f"🤖 Starting test code generation for {report['url']}")
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract test cases from report
        test_cases = report.get("test_cases", {}).get("test_cases", [])
        page_info = report.get("page_info", {})
        url = report.get("url", "")
        
        if not test_cases:
            print("⚠️ No test cases found in report")
            return {"status": "no_test_cases", "files_generated": []}
        
        generated_files = []
        
        # Generate base test configuration
        config_file = await self._generate_test_config(url, page_info, output_dir)
        generated_files.append(config_file)
        
        # Generate conftest.py for pytest fixtures
        conftest_file = await self._generate_conftest(url, page_info, output_dir)
        generated_files.append(conftest_file)
        
        # Generate test files for each test case type
        test_file_map = {}
        
        for test_case in test_cases:
            test_type = test_case.get("type", "unknown")
            
            if test_type not in test_file_map:
                test_file_map[test_type] = []
            
            test_file_map[test_type].append(test_case)
        
        # Generate separate test files for each type
        for test_type, cases in test_file_map.items():
            test_file = await self._generate_test_file(test_type, cases, url, page_info, output_dir)
            generated_files.append(test_file)
        
        # Generate test runner script
        runner_file = await self._generate_test_runner(test_file_map.keys(), output_dir)
        generated_files.append(runner_file)
        
        # Generate README for the test suite
        readme_file = await self._generate_test_readme(report, output_dir)
        generated_files.append(readme_file)
        
        # Generate requirements.txt for test dependencies
        requirements_file = await self._generate_test_requirements(output_dir)
        generated_files.append(requirements_file)
        
        result = {
            "status": "success",
            "files_generated": generated_files,
            "test_types": list(test_file_map.keys()),
            "total_test_cases": len(test_cases),
            "output_directory": output_dir
        }
        
        print(f"✅ Generated {len(generated_files)} test files in '{output_dir}' directory")
        print(f"📊 Test types: {', '.join(test_file_map.keys())}")
        
        return result
    
    async def _generate_test_config(self, url: str, page_info: Dict, output_dir: str) -> str:
        """Generate test configuration file"""
        
        config_content = f'''"""
Test Configuration
Generated on: {datetime.now().isoformat()}
Target URL: {url}
"""

import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class TestConfig:
    """Test configuration settings"""
    
    # Target website
    BASE_URL: str = "{url}"
    
    # Browser settings
    HEADLESS: bool = True
    BROWSER_TIMEOUT: int = 30000  # 30 seconds
    
    # Test settings
    SCREENSHOT_ON_FAILURE: bool = True
    VIDEO_RECORDING: bool = False
    
    # Timeouts
    DEFAULT_TIMEOUT: int = 10000  # 10 seconds
    NAVIGATION_TIMEOUT: int = 30000  # 30 seconds
    
    # Test data
    TEST_DATA_DIR: str = "test_data"
    SCREENSHOTS_DIR: str = "screenshots"
    
    # Page info from analysis
    EXPECTED_TITLE: str = "{page_info.get('title', '')}"
    EXPECTED_LINKS_COUNT: int = {page_info.get('links_count', 0)}
    EXPECTED_IMAGES_COUNT: int = {page_info.get('images_count', 0)}
    EXPECTED_FORMS_COUNT: int = {page_info.get('forms_count', 0)}

# Environment-based overrides
config = TestConfig()

if os.getenv("TEST_HEADLESS") == "false":
    config.HEADLESS = False

if os.getenv("TEST_TIMEOUT"):
    config.DEFAULT_TIMEOUT = int(os.getenv("TEST_TIMEOUT"))

if os.getenv("BASE_URL"):
    config.BASE_URL = os.getenv("BASE_URL")
'''
        
        config_file = os.path.join(output_dir, "test_config.py")
        with open(config_file, "w", encoding="utf-8") as f:
            f.write(config_content)
        
        return config_file
    
    async def _generate_conftest(self, url: str, page_info: Dict, output_dir: str) -> str:
        """Generate pytest conftest.py with fixtures"""
        
        conftest_content = '''"""
Pytest configuration and fixtures
"""

import pytest
import asyncio
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from test_config import config
import os
from datetime import datetime

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def browser():
    """Create browser instance for the test session"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=config.HEADLESS,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
        yield browser
        await browser.close()

@pytest.fixture(scope="function")
async def context(browser: Browser):
    """Create browser context for each test"""
    context = await browser.new_context(
        viewport={'width': 1280, 'height': 720},
        record_video_dir="videos/" if config.VIDEO_RECORDING else None
    )
    yield context
    await context.close()

@pytest.fixture(scope="function")
async def page(context: BrowserContext):
    """Create page for each test"""
    page = await context.new_page()
    
    # Set default timeout
    page.set_default_timeout(config.DEFAULT_TIMEOUT)
    
    # Navigate to base URL
    await page.goto(config.BASE_URL, timeout=config.NAVIGATION_TIMEOUT)
    
    yield page
    
    # Take screenshot on failure
    if config.SCREENSHOT_ON_FAILURE:
        # This will be called even if test fails
        test_name = os.environ.get('PYTEST_CURRENT_TEST', 'unknown_test').split('::')[-1]
        screenshot_path = f"{config.SCREENSHOTS_DIR}/{test_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        os.makedirs(config.SCREENSHOTS_DIR, exist_ok=True)
        try:
            await page.screenshot(path=screenshot_path, full_page=True)
        except:
            pass  # Ignore screenshot errors
    
    await page.close()

@pytest.fixture(scope="session", autouse=True)
def setup_test_directories():
    """Setup test directories"""
    os.makedirs(config.SCREENSHOTS_DIR, exist_ok=True)
    os.makedirs(config.TEST_DATA_DIR, exist_ok=True)
    if config.VIDEO_RECORDING:
        os.makedirs("videos", exist_ok=True)

# Pytest configuration
def pytest_configure(config):
    """Configure pytest"""
    config.addinivalue_line("markers", "slow: marks tests as slow")
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    config.addinivalue_line("markers", "smoke: marks tests as smoke tests")

def pytest_collection_modifyitems(config, items):
    """Modify test collection"""
    for item in items:
        # Add markers based on test names
        if "form" in item.name.lower():
            item.add_marker(pytest.mark.integration)
        if "navigation" in item.name.lower():
            item.add_marker(pytest.mark.smoke)
        if "performance" in item.name.lower():
            item.add_marker(pytest.mark.slow)
'''
        
        conftest_file = os.path.join(output_dir, "conftest.py")
        with open(conftest_file, "w", encoding="utf-8") as f:
            f.write(conftest_content)
        
        return conftest_file
    
    async def _generate_test_file(self, test_type: str, test_cases: List[Dict], url: str, page_info: Dict, output_dir: str) -> str:
        """Generate test file for specific test type"""
        
        prompt = f"""
Generate a comprehensive pytest test file for {test_type} testing using Playwright async API.

Target URL: {url}
Page Info: {json.dumps(page_info, indent=2)}

Test Cases to implement:
{json.dumps(test_cases, indent=2)}

Requirements:
1. Use Playwright async API with pytest-asyncio
2. Import fixtures from conftest.py (browser, context, page)
3. Import config from test_config.py
4. Create comprehensive test methods for each test case
5. Include proper assertions and error handling
6. Add docstrings and comments
7. Use pytest markers (@pytest.mark.asyncio, @pytest.mark.smoke, etc.)
8. Handle timeouts and waits appropriately
9. Include data-driven tests where applicable
10. Add helper methods for common operations

For form testing:
- Test form validation (required fields, input types)
- Test form submission with valid/invalid data
- Test form accessibility (keyboard navigation)
- Test form responsiveness

For navigation testing:
- Test all links functionality
- Test internal vs external links
- Test link accessibility
- Verify proper HTTP status codes

For media testing:
- Test image loading
- Test alt-text presence
- Test responsive images
- Test lazy loading if present

For performance testing:
- Test page load times
- Test Core Web Vitals
- Test resource loading

For accessibility testing:
- Test keyboard navigation
- Test screen reader compatibility
- Test color contrast
- Test ARIA labels

Generate clean, production-ready Python code with proper error handling and logging.
Use modern Python async/await patterns.
Include type hints where appropriate.
"""
        
        messages = [
            SystemMessage(content="You are an expert test automation engineer. Generate high-quality, maintainable test code using Playwright and pytest. Focus on best practices, proper error handling, and comprehensive test coverage."),
            HumanMessage(content=prompt)
        ]
        
        response = self.llm.invoke(messages)
        test_code = response.content
        
        # Clean up the response to extract just the Python code
        if "```python" in test_code:
            test_code = test_code.split("```python")[1].split("```")[0].strip()
        elif "```" in test_code:
            test_code = test_code.split("```")[1].split("```")[0].strip()
        
        # Generate filename
        filename = f"test_{test_type}.py"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(test_code)
        
        return filepath
    
    async def _generate_test_runner(self, test_types: List[str], output_dir: str) -> str:
        """Generate test runner script"""
        
        runner_content = f'''#!/usr/bin/env python3
"""
Test Runner Script
Generated on: {datetime.now().isoformat()}

This script provides convenient ways to run the generated tests.
"""

import subprocess
import sys
import os
import argparse
from pathlib import Path

def run_command(cmd, description=""):
    """Run a command and handle errors"""
    print(f"🚀 {{description or cmd}}")
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
        if result.stdout:
            print(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print(f"❌ Error: {{e}}")
        if e.stdout:
            print(f"STDOUT: {{e.stdout}}")
        if e.stderr:
            print(f"STDERR: {{e.stderr}}")
        return False

def install_dependencies():
    """Install test dependencies"""
    print("📦 Installing test dependencies...")
    return run_command("pip install -r requirements.txt", "Installing dependencies")

def run_all_tests():
    """Run all tests"""
    cmd = "python -m pytest -v --tb=short"
    return run_command(cmd, "Running all tests")

def run_smoke_tests():
    """Run smoke tests only"""
    cmd = "python -m pytest -v -m smoke --tb=short"
    return run_command(cmd, "Running smoke tests")

def run_integration_tests():
    """Run integration tests only"""
    cmd = "python -m pytest -v -m integration --tb=short"
    return run_command(cmd, "Running integration tests")

def run_specific_test_type(test_type):
    """Run tests for specific type"""
    cmd = f"python -m pytest -v test_{{test_type}}.py --tb=short"
    return run_command(cmd, f"Running {{test_type}} tests")

def run_with_html_report():
    """Run tests and generate HTML report"""
    cmd = "python -m pytest -v --html=report.html --self-contained-html"
    return run_command(cmd, "Running tests with HTML report")

def main():
    parser = argparse.ArgumentParser(description="Test Runner for Generated Tests")
    parser.add_argument("--install", action="store_true", help="Install dependencies")
    parser.add_argument("--smoke", action="store_true", help="Run smoke tests only")
    parser.add_argument("--integration", action="store_true", help="Run integration tests only")
    parser.add_argument("--type", choices={list(test_types)}, help="Run specific test type")
    parser.add_argument("--html", action="store_true", help="Generate HTML report")
    parser.add_argument("--all", action="store_true", help="Run all tests (default)")
    
    args = parser.parse_args()
    
    # Change to test directory
    test_dir = Path(__file__).parent
    os.chdir(test_dir)
    
    success = True
    
    if args.install:
        success = install_dependencies()
    elif args.smoke:
        success = run_smoke_tests()
    elif args.integration:
        success = run_integration_tests()
    elif args.type:
        success = run_specific_test_type(args.type)
    elif args.html:
        success = run_with_html_report()
    else:
        success = run_all_tests()
    
    if success:
        print("✅ Test execution completed successfully!")
    else:
        print("❌ Test execution failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
        
        runner_file = os.path.join(output_dir, "run_tests.py")
        with open(runner_file, "w", encoding="utf-8") as f:
            f.write(runner_content)
        
        # Make it executable
        os.chmod(runner_file, 0o755)
        
        return runner_file
    
    async def _generate_test_readme(self, report: Dict[str, Any], output_dir: str) -> str:
        """Generate README for the test suite"""
        
        test_cases = report.get("test_cases", {}).get("test_cases", [])
        page_info = report.get("page_info", {})
        url = report.get("url", "")
        
        readme_content = f'''# Automated Test Suite

Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Target URL: {url}

## Overview

This test suite was automatically generated based on the analysis of the target website.
It includes comprehensive tests for various aspects of the web application.

## Target Website Information

- **URL**: {url}
- **Title**: {page_info.get('title', 'N/A')}
- **Links**: {page_info.get('links_count', 0)}
- **Images**: {page_info.get('images_count', 0)}
- **Forms**: {page_info.get('forms_count', 0)}

## Test Cases Generated

Total test cases: {len(test_cases)}

'''
        
        # Add test case details
        for i, tc in enumerate(test_cases, 1):
            readme_content += f'''### {i}. {tc.get('title', 'Unknown Test')}

- **Type**: {tc.get('type', 'unknown')}
- **Priority**: {tc.get('priority', 'medium')}
- **Estimated Time**: {tc.get('estimated_time', 'N/A')}
- **Description**: {tc.get('description', 'No description')}

'''
        
        readme_content += '''## Installation

1. Install Python dependencies:
```bash
pip install -r requirements.txt
```

2. Install Playwright browsers:
```bash
playwright install
```

## Running Tests

### Quick Start
```bash
# Run all tests
python run_tests.py

# Install dependencies first
python run_tests.py --install

# Run smoke tests only
python run_tests.py --smoke

# Run integration tests only
python run_tests.py --integration

# Generate HTML report
python run_tests.py --html
```

### Using pytest directly
```bash
# Run all tests
pytest -v

# Run specific test file
pytest test_form_validation.py -v

# Run with markers
pytest -m smoke -v
pytest -m integration -v

# Generate HTML report
pytest --html=report.html --self-contained-html
```

## Test Configuration

Edit `test_config.py` to modify test settings:

- `BASE_URL`: Target website URL
- `HEADLESS`: Run browser in headless mode
- `BROWSER_TIMEOUT`: Browser operation timeout
- `SCREENSHOT_ON_FAILURE`: Take screenshots on test failures

## Environment Variables

You can override configuration using environment variables:

```bash
export TEST_HEADLESS=false  # Run with browser GUI
export TEST_TIMEOUT=20000   # Set custom timeout
export BASE_URL=https://example.com  # Override target URL
```

## Test Structure

- `conftest.py` - Pytest fixtures and configuration
- `test_config.py` - Test configuration settings
- `test_*.py` - Individual test files by category
- `run_tests.py` - Test runner script
- `requirements.txt` - Python dependencies

## Test Reports

- Screenshots are saved to `screenshots/` directory on test failures
- HTML reports can be generated with `--html` option
- Video recordings (if enabled) are saved to `videos/` directory

## Troubleshooting

### Common Issues

1. **Browser not found**: Run `playwright install`
2. **Timeout errors**: Increase timeout in `test_config.py`
3. **Permission errors**: Ensure proper file permissions

### Debug Mode

Run tests with debug output:
```bash
pytest -v -s --tb=long
```

## Contributing

This test suite was automatically generated. To modify tests:

1. Edit the individual test files
2. Update `test_config.py` for configuration changes
3. Modify `conftest.py` for fixture changes

## Support

For issues with the generated tests, check:
- Test configuration in `test_config.py`
- Browser compatibility
- Network connectivity to target URL
'''
        
        readme_file = os.path.join(output_dir, "README.md")
        with open(readme_file, "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        return readme_file
    
    async def _generate_test_requirements(self, output_dir: str) -> str:
        """Generate requirements.txt for test dependencies"""
        
        requirements_content = '''# Test automation dependencies
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-html>=3.2.0
playwright>=1.40.0
pytest-xdist>=3.3.0  # For parallel test execution
pytest-cov>=4.1.0    # For coverage reports
pytest-mock>=3.11.0  # For mocking
allure-pytest>=2.13.0  # For Allure reports (optional)

# Additional utilities
requests>=2.31.0
beautifulsoup4>=4.12.0
Pillow>=10.0.0  # For image processing in tests
selenium>=4.15.0  # Alternative to Playwright (optional)

# Development dependencies
black>=23.0.0  # Code formatting
flake8>=6.0.0  # Linting
mypy>=1.5.0    # Type checking
'''
        
        requirements_file = os.path.join(output_dir, "requirements.txt")
        with open(requirements_file, "w", encoding="utf-8") as f:
            f.write(requirements_content)
        
        return requirements_file


async def main():
    """Demo function to test the test code generator"""
    
    # Load a sample report
    report_file = "reports/report_books.toscrape.com_20250604_155126.json"
    
    if not os.path.exists(report_file):
        print(f"❌ Report file not found: {report_file}")
        print("Please run the web analysis first to generate a report.")
        return
    
    with open(report_file, "r", encoding="utf-8") as f:
        report = json.load(f)
    
    # Generate test code
    generator = TestCodeGenerator()
    result = await generator.generate_test_code(report)
    
    print(f"\n{'='*50}")
    print("📊 TEST CODE GENERATION RESULTS")
    print(f"{'='*50}")
    print(f"Status: {result['status']}")
    print(f"Files generated: {len(result['files_generated'])}")
    print(f"Test types: {', '.join(result['test_types'])}")
    print(f"Total test cases: {result['total_test_cases']}")
    print(f"Output directory: {result['output_directory']}")
    
    print(f"\n📁 Generated files:")
    for file_path in result['files_generated']:
        print(f"   • {file_path}")
    
    print(f"\n🚀 To run the generated tests:")
    print(f"   cd {result['output_directory']}")
    print(f"   python run_tests.py --install")
    print(f"   python run_tests.py")


if __name__ == "__main__":
    asyncio.run(main()) 